{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8413fff2-e2fa-4507-b368-38f2f89060ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 13 10:51:32 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0    76W /  70W |   5116MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8823a7f6-a560-4591-aef1-f49725f1e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from small_simple_ae import SimpleAE\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb63e638-356e-4db2-bf48-079aed209932",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c939c4-e85f-400c-ab20-e4243cd60dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/app/data/anime/')\n",
    "data_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294e9b26-d3b1-4206-b4ed-5cf95134b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_c = 3\n",
    "image_size = 64\n",
    "\n",
    "dataset = dset.ImageFolder(root=data_dir,\n",
    "                           transform=T.Compose([\n",
    "                               T.Resize(image_size),\n",
    "                               T.CenterCrop(image_size),\n",
    "                               T.ToTensor(),\n",
    "                               T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16817cad-0f21-436d-babb-ef8270cb670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7589fa34-d21d-403e-808d-52ec026b6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, T = 1000):\n",
    "        super(DDPM, self).__init__()\n",
    "        \n",
    "        self.diffuser = SimpleAE(in_channels=3, filters=128)\n",
    "\n",
    "        self.T = T\n",
    "        self.betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "        self.a = 1. - self.betas\n",
    "        self.a_hat = torch.cumprod(self.a, dim=0)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        t = torch.randint(self.T, (x0.size(0),)).to(device)\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        \n",
    "        a_hat_t = self.a_hat[t][:, None, None, None].to(device)\n",
    "        \n",
    "        pred_eps = self.diffuser(torch.sqrt(a_hat_t) * x0 +\n",
    "                                 torch.sqrt(1 - a_hat_t) * eps, t.unsqueeze(1).float()/self.T)\n",
    "        losses_dict = self.diffuser.loss_function(pred_eps, eps)\n",
    "        return losses_dict\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples, data_size):\n",
    "        xT = torch.randn([n_samples, *data_size]).to(device)\n",
    "        for t in tqdm(range(self.T, 0, -1)):\n",
    "            z = torch.randn_like(xT) if t > 1 else 0\n",
    "            k = ((1 - self.a[t-1])/math.sqrt(1 - self.a_hat[t-1])).to(xT.device)\n",
    "            t_tensor = t*torch.ones((n_samples, 1)).to(xT.device)/self.T\n",
    "            sigma = torch.sqrt(1 - self.a[t-1]).to(xT.device)\n",
    "            xT = 1/math.sqrt(self.a[t-1]) * (xT - k*self.diffuser(xT, t_tensor)) + z * sigma\n",
    "        return xT\n",
    "    \n",
    "def log_images(t, i):\n",
    "    img_grid = make_grid(samples, nrow=4, value_range=(-1, 1))\n",
    "    save_image(img_grid, f\"log_images/{i}_sample.png\")\n",
    "    if wandb.run is not None:\n",
    "        images = wandb.Image(img_grid, caption=\"Random sampled images\")\n",
    "\n",
    "        wandb.log({\"Samples\": images, 'i': i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ba8088-c148-43c0-9b7a-9c6bccae2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnerlfield\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/notebooks/wandb/run-20221113_105135-2aw5x3p8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nerlfield/anime_faces_diffusion/runs/2aw5x3p8\" target=\"_blank\">ddpm_samll-simple-ae_latent-64_diffusion</a></strong> to <a href=\"https://wandb.ai/nerlfield/anime_faces_diffusion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nerlfield/anime_faces_diffusion/runs/2aw5x3p8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f61f0ed0d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='anime_faces_diffusion', name=f'ddpm_samll-simple-ae_latent-64_diffusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef771b4-78eb-4b07-86ae-0ae81abb50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPM().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efcb7f-213d-43b9-a1e0-ca16b963822d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:30<00:00, 11.05it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 500\n",
    "loss_vis_freq = 200\n",
    "\n",
    "i = 0\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epoch, num_epoch, 1):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch[0].to(device) #only images\n",
    "        losses_dict = model(batch)\n",
    "        \n",
    "        loss = losses_dict['loss']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({'loss': loss.item(),\n",
    "                       'i': i})\n",
    "        elif i % loss_vis_freq == 0:\n",
    "            print(loss.item())\n",
    "        \n",
    "        i += 1\n",
    "        # break\n",
    "    \n",
    "    model.eval()\n",
    "    samples = model.sample(32, (image_c, image_size, image_size))\n",
    "    log_images(samples, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651731d-95fd-43fa-a765-002006b72041",
   "metadata": {},
   "source": [
    "# overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f404e0-24be-43bc-9051-aae43bd7b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     batch = batch[0].to(device)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b378c8e-5cfb-4c89-84e0-fff0c0b79290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epoch = 1000\n",
    "# loss_vis_freq = 200\n",
    "\n",
    "# i = 0\n",
    "# for epoch in range(num_epoch):\n",
    "#     model.train()\n",
    "#     for wsefd in range(1000):\n",
    "#         losses_dict = model(batch)\n",
    "\n",
    "#         loss = losses_dict['loss']\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if wandb.run is not None:\n",
    "#             wandb.log({'loss': loss.item(),\n",
    "#                        'i': i})\n",
    "#         elif i % loss_vis_freq == 0:\n",
    "#             print(loss.item())\n",
    "\n",
    "#         i += 1\n",
    "        \n",
    "#     model.eval()\n",
    "#     samples = model.sample(32, (1, 28, 28))\n",
    "#     log_images(samples, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
