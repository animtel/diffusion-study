{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594e0993-d025-4cbb-9c40-d9751487e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers import logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c864a78-ef22-484a-a055-0150c7c159c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993bc6b-b066-4de5-957f-d42e1e608383",
   "metadata": {},
   "source": [
    "# Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56b3ab2-6dde-46dc-ab46-6e15cd44e85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path_to_checkpoint = 'CompVis/stable-diffusion-v1-4'\n",
    "path_to_checkpoint = 'hakurei/waifu-diffusion'\n",
    "\n",
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(path_to_checkpoint, subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "#@title Set up the Tokenizer and the Text Encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    path_to_checkpoint,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    path_to_checkpoint, subfolder=\"text_encoder\"#, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(path_to_checkpoint, subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1722f-8792-452e-911a-5cc9fd54f203",
   "metadata": {},
   "source": [
    "# Download CLIP embeddings and add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "500aeb1a-04f5-46e2-b95c-b8fdff061ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02113962173461914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 15,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 3819,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b87c22b8f640bcb55106aeed00a5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02222895622253418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 15,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 12,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345c3299ed9f4ecfb4568200c5ec0f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/12.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## The placeholder token for your concept is `<wlop-style>`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "#@title Load your concept here\n",
    "#@markdown Enter the `repo_id` for a concept you like (you can find pre-learned concepts in the public [SD Concepts Library](https://huggingface.co/sd-concepts-library))\n",
    "# repo_id_embeds = \"sd-concepts-library/arcane-face\" #@param {type:\"string\"}\n",
    "# repo_id_embeds = \"sd-concepts-library/arcane-style-jv\" #@param {type:\"string\"}\n",
    "repo_id_embeds = \"sd-concepts-library/wlop-style\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown (Optional) in case you have a `learned_embeds.bin` file and not a `repo_id`, add the path to `learned_embeds.bin` to the `embeds_url` variable \n",
    "embeds_url = \"\" #Add the URL or path to a learned_embeds.bin file in case you have one\n",
    "placeholder_token_string = \"\" #Add what is the token string in case you are uploading your own embed\n",
    "\n",
    "downloaded_embedding_folder = \"./downloaded_embedding\"\n",
    "if not os.path.exists(downloaded_embedding_folder):\n",
    "    os.mkdir(downloaded_embedding_folder)\n",
    "if(not embeds_url):\n",
    "    embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"learned_embeds.bin\")\n",
    "    token_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"token_identifier.txt\")\n",
    "    !cp $embeds_path $downloaded_embedding_folder\n",
    "    !cp $token_path $downloaded_embedding_folder\n",
    "    with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:\n",
    "        placeholder_token_string = file.read()\n",
    "else:\n",
    "    !wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url\n",
    "\n",
    "learned_embeds_path = f\"{downloaded_embedding_folder}/learned_embeds.bin\"\n",
    "\n",
    "display (Markdown(\"## The placeholder token for your concept is `%s`\"%(placeholder_token_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "054ae5bc-fa84-4917-bc95-e4fe96ea1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the newly learned embeddings into CLIP\n",
    "def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
    "    loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
    "\n",
    "    # separate token and the embeds\n",
    "    trained_token = list(loaded_learned_embeds.keys())[0]\n",
    "    embeds = loaded_learned_embeds[trained_token]\n",
    "\n",
    "    # cast to dtype of text_encoder\n",
    "    dtype = text_encoder.get_input_embeddings().weight.dtype\n",
    "    embeds.to(dtype)\n",
    "\n",
    "    # add the token in tokenizer\n",
    "    token = token if token is not None else trained_token\n",
    "    num_added_tokens = tokenizer.add_tokens(token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n",
    "  \n",
    "  # resize the token embeddings\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # get the id for the token and assign the embeds\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
    "    \n",
    "\n",
    "load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2adf55-3a56-446c-8acd-a6bdd6c785f5",
   "metadata": {},
   "source": [
    "# Diffusion loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e91551e-bb41-4a19-a875-e6a35a779808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        t_img = tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1\n",
    "        if t_img.shape[1] == 4:\n",
    "            t_img = t_img[:, :3]\n",
    "        latent = vae.encode(t_img) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8712da4f-2d96-4087-ab16-4e6520a85a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the image with PIL\n",
    "input_image = Image.open('oles1.jpg').resize((512, 512))\n",
    "encoded = pil_to_latent(input_image)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d3cb0-e892-404d-bc97-27ee6745649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022066116333007812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 15,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf9dde9d6df462d8f12dbcdfc378451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings (same as before except for the new prompt)\n",
    "# prompt = [\"original, 1girl, solo, simple background, backlighting, rain, night, depth of field\"]\n",
    "# prompt = [\"<arcane-face>, ultra realistic, high resolution, high detailed\"]\n",
    "prompt = [\"<wlop-style>\"]\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                         # default width of Stable Diffusion\n",
    "num_inference_steps = 100            # Number of denoising steps\n",
    "guidance_scale = 8                   # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(43)   # Seed generator to create the inital latent noise\n",
    "batch_size = 1\n",
    "start_step = 50\n",
    "\n",
    "\n",
    "# Prep text (same as before)\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "# Prep Scheduler (setting the number of inference steps)\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Prep latents (noising appropriately for start_step)\n",
    "start_sigma = scheduler.sigmas[start_step]\n",
    "noise = torch.randn_like(encoded)\n",
    "latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "latents = latents.to(torch_device).float()\n",
    "\n",
    "# Loop\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    if i >= start_step: # << This is the only modification to the loop we do\n",
    "        \n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8569f-b39d-4bfc-bd2e-3ff721b6e827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
