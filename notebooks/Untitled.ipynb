{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36cd72b-804b-49fd-bcc4-c7e20054bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c40e4b-4731-4518-96c3-a49768db5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/app/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28310a8-22fa-488c-a102-143583f240f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a9019a-d2a1-4769-946a-2ddce8d979d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mUNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_res_blocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattention_resolutions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchannel_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconv_resample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads_upsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_scale_shift_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "The full UNet model with attention and timestep embedding.\n",
       "\n",
       ":param in_channels: channels in the input Tensor.\n",
       ":param model_channels: base channel count for the model.\n",
       ":param out_channels: channels in the output Tensor.\n",
       ":param num_res_blocks: number of residual blocks per downsample.\n",
       ":param attention_resolutions: a collection of downsample rates at which\n",
       "    attention will take place. May be a set, list, or tuple.\n",
       "    For example, if this contains 4, then at 4x downsampling, attention\n",
       "    will be used.\n",
       ":param dropout: the dropout probability.\n",
       ":param channel_mult: channel multiplier for each level of the UNet.\n",
       ":param conv_resample: if True, use learned convolutions for upsampling and\n",
       "    downsampling.\n",
       ":param dims: determines if the signal is 1D, 2D, or 3D.\n",
       ":param num_classes: if specified (as an int), then this model will be\n",
       "    class-conditional with `num_classes` classes.\n",
       ":param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
       ":param num_heads: the number of attention heads in each attention layer.\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /app/src/unet.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     SuperResModel\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "UNetModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3cf2c4-46d8-4495-bae2-40528451fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ds = []\n",
    "for res in \"16, 8\".split(\",\"):\n",
    "    attention_ds.append(64 // int(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8f7586-11c0-41b7-ae13-5d64e5b2bc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e1ecd65-2d67-4b92-bca4-9105cee7eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNetModel(in_channels=3, model_channels=64, num_heads=4, channel_mult = (1, 2, 3, 4), out_channels=3, num_res_blocks=4, attention_resolutions=[4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83402ccf-cb37-402a-84cf-94bd968645bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3, 64, 64)\n",
    "t = torch.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70a270d-32a9-4f0f-818b-0ade750d49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = unet_model(a, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd4acc7-2b5e-4698-8bfd-36b971310498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~(y==0.)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f03fa0-5f7e-4b4f-b24e-8d23aacd4d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
